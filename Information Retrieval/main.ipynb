{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR4_Doc_Ret.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xKpt7cjEwMc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqHo4rqrCkYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46eb4407-22e3-41a3-f8d3-9eb5b4e6ab6c"
      },
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')  \n",
        " nltk.download('punkt')\n",
        " !pip install num2words\n",
        " !pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Wn78iMDDL0"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from num2words import num2words\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "\n",
        "import spacy\n",
        "from gensim.summarization.bm25 import BM25\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
        "import concurrent.futures\n",
        "import itertools\n",
        "import operator\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEOxBy-SDcGx"
      },
      "source": [
        "## Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA4AKVTwDIl2"
      },
      "source": [
        "class PreProcessor:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data;\n",
        "\n",
        "    def execute(self):\n",
        "        self.convert_lower_case()\n",
        "        self.remove_punctuation() #remove comma seperately\n",
        "        self.remove_apostrophe()\n",
        "        self.remove_stop_words()\n",
        "        self.convert_numbers()\n",
        "        self.stemming()\n",
        "        self.remove_punctuation()\n",
        "        self.convert_numbers()\n",
        "        self.stemming() #needed again as we need to stem the words\n",
        "        self.remove_punctuation() #needed again as num2word is giving few hypens and commas fourty-one\n",
        "        self.remove_stop_words() #needed again as num2word is giving stop words 101 - one hundred and one        \n",
        "        return self.data\n",
        "\n",
        "    def convert_lower_case(self):\n",
        "        self.data = np.char.lower(self.data)\n",
        "\n",
        "    def remove_stop_words(self):\n",
        "        stop_words = stopwords.words('english')\n",
        "        words = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in words:\n",
        "            if w not in stop_words and len(w) > 1:\n",
        "                new_text = new_text + \" \" + w\n",
        "        self.data = new_text\n",
        "    \n",
        "    def remove_punctuation(self):\n",
        "        symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "        for i in range(len(symbols)):\n",
        "            data = np.char.replace(self.data, symbols[i], ' ')\n",
        "            data = np.char.replace(data, \"  \", \" \")\n",
        "        data = np.char.replace(data, ',', '')\n",
        "        self.data = data\n",
        "\n",
        "    def remove_apostrophe(self):\n",
        "        self.data = np.char.replace(self.data, \"'\", \"\")\n",
        "\n",
        "    def stemming(self):\n",
        "        stemmer= PorterStemmer()\n",
        "        \n",
        "        tokens = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in tokens:\n",
        "            new_text = new_text + \" \" + stemmer.stem(w)\n",
        "        self.data = new_text\n",
        "\n",
        "    def convert_numbers(self):\n",
        "        tokens = word_tokenize(str(self.data))\n",
        "        new_text = \"\"\n",
        "        for w in tokens:\n",
        "            try:\n",
        "                w = num2words(int(w))\n",
        "            except:\n",
        "                a = 0\n",
        "            new_text = new_text + \" \" + w\n",
        "        new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "        self.data = new_text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5oydBENDgKa"
      },
      "source": [
        "## Compressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz9pkYi4DRo6"
      },
      "source": [
        "class Compressor:\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df;\n",
        "        self.col = ['TITLE', 'DESCRIPTION', 'BULLET_POINTS', 'BRAND', 'BROWSE_NODE_ID']\n",
        "        self.data_dict = []\n",
        "\n",
        "    def validRow(self, row):\n",
        "        c = row.shape[0]\n",
        "        for i in range(c):\n",
        "            if type(row[i]) is float:\n",
        "                row[i] = ''\n",
        "        return row\n",
        "\n",
        "    def compress(self):\n",
        "        print(f'[INFO] Compress init')\n",
        "        self.df.fillna('', inplace=True) # Handling null values\n",
        "\n",
        "        data = {}\n",
        "        l = self.df.shape[0]\n",
        "        x = int(l/100);\n",
        "        for i in range(l):\n",
        "            if i%x == 0:\n",
        "                print('.' , end='')\n",
        "            bni = self.df.loc[i]['BROWSE_NODE_ID']\n",
        "            data[bni] = [\"\", \"\", \"\", \"\", bni]\n",
        "        print()\n",
        "        for i in range(self.df.shape[0]):\n",
        "            if i%x == 0:\n",
        "                print('|' , end='')\n",
        "            bni = self.df.loc[i]['BROWSE_NODE_ID']\n",
        "            row = self.validRow(self.df.loc[i])\n",
        "\n",
        "            data[bni][0] = data[bni][0] + ' \\n ' + row['TITLE']\n",
        "            data[bni][1] = data[bni][1] + ' \\n ' + row['DESCRIPTION']\n",
        "            data[bni][2] = data[bni][2] + ' \\n ' + row['BULLET_POINTS']\n",
        "            data[bni][3] = data[bni][3] + ' \\n ' + row['BRAND']\n",
        "        print()\n",
        "        self.create_new_df(data)\n",
        "\n",
        "    # precompress will preprocess text while compressing it\n",
        "    def precompress(self):\n",
        "        print(f'[INFO] Compress init')\n",
        "        self.df.fillna('', inplace=True) # Handling null values\n",
        "\n",
        "        data = {}\n",
        "        l = self.df.shape[0]\n",
        "        x = int(l/100);\n",
        "        for i in range(l):\n",
        "            if i%x == 0:\n",
        "                print('.' , end='')\n",
        "            bni = self.df.loc[i]['BROWSE_NODE_ID']\n",
        "            data[bni] = [\"\", \"\", \"\", \"\", bni]\n",
        "        print()\n",
        "        for i in range(self.df.shape[0]):\n",
        "            if i%x == 0:\n",
        "                print('|' , end='')\n",
        "            bni = self.df.loc[i]['BROWSE_NODE_ID']\n",
        "            row = self.validRow(self.df.loc[i])\n",
        "\n",
        "            data[bni][0] = data[bni][0] + ' \\n ' + PreProcessor(row['TITLE']).execute()\n",
        "            data[bni][1] = data[bni][1] + ' \\n ' + PreProcessor(row['DESCRIPTION']).execute()\n",
        "            data[bni][2] = data[bni][2] + ' \\n ' + PreProcessor(row['BULLET_POINTS']).execute()\n",
        "            data[bni][3] = data[bni][3] + ' \\n ' + PreProcessor(row['BRAND']).execute()\n",
        "        print()\n",
        "        self.create_new_df(data)\n",
        "\n",
        "    def create_new_df(self, data):\n",
        "        print(f'[INFO] Creating new dataframe')\n",
        "        for d in data:\n",
        "            new_row = {self.col[0] : data[d][0], self.col[1] : data[d][1], self.col[2] : data[d][2], self.col[3] : data[d][3], self.col[4] : data[d][4]}\n",
        "            self.data_dict.append(new_row)\n",
        "        print(f'[INFO] data compressed')\n",
        "\n",
        "    def save_csv(self, filename):\n",
        "        with open(filename, 'w') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=self.col)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(self.data_dict)\n",
        "\n",
        "    def get_csv(self):\n",
        "        return pd.DataFrame.from_dict(self.data_dict)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NONVUE1LEW1D"
      },
      "source": [
        "## Nodes Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1uH4OOpEbAM"
      },
      "source": [
        "SPACY_MODEL = os.environ.get('SPACY_MODEL', 'en_core_web_sm')\n",
        "nlp = spacy.load(SPACY_MODEL, disable=['ner', 'parser', 'textcat'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6m1AtCaEkJV"
      },
      "source": [
        "class NodeExtractor:\n",
        "\n",
        "    def __init__(self, nlp):\n",
        "        self.tokenize = lambda text: [token.lemma_ for token in nlp(text)]\n",
        "        self.bm25 = None\n",
        "        self.passages = None\n",
        "        self.directory = None # {1: 1, 2:1, 3:1, 4:2 ...... } key = passageindex & value = column index\n",
        "\n",
        "    def preprocess(self, df):\n",
        "        print(f'[INFO] compiling data')\n",
        "        directory = {}\n",
        "        passages = []\n",
        "        l = df.shape[0]\n",
        "        x = int(l/100)\n",
        "        for i in range(100):\n",
        "            print('.', end='')\n",
        "        print()\n",
        "        for i in range(l):\n",
        "            if i%x == 0:\n",
        "                print('|', end='')\n",
        "            txt = f'{df.loc[i][\"TITLE\"]} \\n {df.loc[i][\"DESCRIPTION\"]} \\n {df.loc[i][\"BULLET_POINTS\"]} \\n {df.loc[i][\"BRAND\"]}'    \n",
        "            sentences = txt.split('\\n')\n",
        "            for j in range(len(sentences)):\n",
        "                sentences[j] = sentences[j].strip()\n",
        "                if (len(sentences[j]) < 1):\n",
        "                    continue\n",
        "                passages.append(sentences[j])\n",
        "                directory[len(passages)-1] = df['BROWSE_NODE_ID'].loc[i]\n",
        "        print()\n",
        "        self.passages = passages\n",
        "        self.directory = directory\n",
        "    \n",
        "    def train(self, df):\n",
        "        print(f'[INFO] Training init')\n",
        "        self.preprocess(df)\n",
        "        print(f'[INFO] Featching corpus to BM25....')\n",
        "        corpus = [self.tokenize(p) for p in self.passages]\n",
        "        self.bm25 = BM25(corpus)\n",
        "        print(f'[INFO] Training finished')\n",
        "\n",
        "    def test(self, row, topn=10):\n",
        "        text = f'{row[\"TITLE\"]} \\n {row[\"DESCRIPTION\"]} \\n {row[\"BULLET_POINTS\"]} \\n {row[\"BRAND\"]}'\n",
        "        text = PreProcessor(text).execute()\n",
        "        tokens = self.tokenize(text)\n",
        "        average_idf = sum(map(lambda k: float(self.bm25.idf[k]), self.bm25.idf.keys()))\n",
        "        scores = self.bm25.get_scores(tokens, average_idf)\n",
        "        pairs = [(s, i) for i, s in enumerate(scores)]\n",
        "        pairs.sort(reverse=True)\n",
        "        indices = [self.directory[i] for _, i in pairs[:topn]]\n",
        "        scores = [s for s, _ in pairs[:topn]]\n",
        "        return [scores, indices]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1uuZdyKGnIg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "7JCNOUC_EsJ_",
        "outputId": "7c034730-0fe8-4c9b-ff28-00e4f2f5f695"
      },
      "source": [
        "data = pd.read_csv('s_train.csv')\n",
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>BRAND</th>\n",
              "      <th>BROWSE_NODE_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Pete The Cat Bedtime Blues Doll, 14.5 Inch</td>\n",
              "      <td>Pete the Cat is the coolest, most popular cat ...</td>\n",
              "      <td>[Pete the Cat Bedtime Blues plush doll,Based o...</td>\n",
              "      <td>MerryMakers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The New Yorker NYHM014 Refrigerator Magnet, 2 ...</td>\n",
              "      <td>The New Yorker Handsome Cello Wrapped Hard Mag...</td>\n",
              "      <td>[Cat In A Tea Cup by New Yorker cover artist G...</td>\n",
              "      <td>The New Yorker</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>The Ultimate Self-Sufficiency Handbook: A Comp...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Skyhorse Publishing</td>\n",
              "      <td>imusti</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Amway Nutrilite Kids Chewable Iron Tablets (100)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Nutrilite Kids,Chewable Iron Tablets,Quantity...</td>\n",
              "      <td>Amway</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Teacher Planner Company A4 6 Lesson Academic T...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... BROWSE_NODE_ID\n",
              "0           0  ...              0\n",
              "1           1  ...              1\n",
              "2           2  ...              2\n",
              "3           3  ...              3\n",
              "4           4  ...              4\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJvrMtEfG489",
        "outputId": "9aab89aa-2b84-4873-b9f4-b026077056b4"
      },
      "source": [
        "compressor = Compressor(data)\n",
        "compressor.precompress()\n",
        "compressed_data = compressor.get_csv()\n",
        "# compressed_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Compress init\n",
            "....................................................................................................\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "[INFO] Creating new dataframe\n",
            "[INFO] data compressed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-HeJ2dhHji6",
        "outputId": "f33b8a66-48c1-4043-c64e-a057d0bf5a8c"
      },
      "source": [
        "model = NodeExtractor(nlp)\n",
        "model.train(compressed_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Training init\n",
            "[INFO] compiling data\n",
            "....................................................................................................\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "[INFO] Featching corpus to BM25....\n",
            "[INFO] Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfVlBZ2qKry7"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "OdYkM2ETIGEI",
        "outputId": "a9fcc855-dfa4-4ba8-e8a4-8edb06a6b37a"
      },
      "source": [
        "# test_data = pd.read_csv('/content/drive/MyDrive/AmazonMLChallenge/Dataset/test.csv', escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE, engine='python')\n",
        "test_data = pd.read_csv('m_test.csv')\n",
        "test_data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>BRAND</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Command 3M Small Kitchen Hooks, White, Decorat...</td>\n",
              "      <td>Sale Unit: PACK</td>\n",
              "      <td>[INCLUDES - 9 hooks and 12 small indoor strips...</td>\n",
              "      <td>Command</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>O'Neal Jump Hardware JAG Unisex-Adult Glove (B...</td>\n",
              "      <td>Synthetic leather palm with double-layer thumb...</td>\n",
              "      <td>[Silicone printing for a better grip. Long las...</td>\n",
              "      <td>O'Neal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>NFL Detroit Lions Portable Party Fridge, 15.8 ...</td>\n",
              "      <td>Boelter Brands lets you celebrate your favorit...</td>\n",
              "      <td>[Runs on 12 Volt DC Power or 110 Volt AC Power...</td>\n",
              "      <td>Boelter Brands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>Panasonic Single Line KX-TS880MX Corded Phone ...</td>\n",
              "      <td>Features: 50 Station Phonebook Corded Phone Al...</td>\n",
              "      <td>Panasonic Landline Phones doesn't come with a ...</td>\n",
              "      <td>Panasonic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>Zero Baby Girl's 100% Cotton Innerwear Bloomer...</td>\n",
              "      <td>Zero Baby Girl Panties Set. 100% Cotton, Breat...</td>\n",
              "      <td>[Zero Baby Girl Panties, Pack of 6, 100% Cotto...</td>\n",
              "      <td>Zero</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...           BRAND\n",
              "0           0  ...         Command\n",
              "1           1  ...          O'Neal\n",
              "2           2  ...  Boelter Brands\n",
              "3           3  ...       Panasonic\n",
              "4           4  ...            Zero\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfI243v-W3i9"
      },
      "source": [
        "Multiclass Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64eU98xWW_lt",
        "outputId": "a3d4174c-d3d2-466b-ad36-3c9d32890d8b"
      },
      "source": [
        "out = []\n",
        "l = test_data.shape[0]\n",
        "x = int(l/100)\n",
        "\n",
        "for i in range(100):\n",
        "    print('.', end='')\n",
        "print()\n",
        "\n",
        "for i in range(l):\n",
        "    if i%x==0:\n",
        "        print('|', end='')\n",
        "    row = test_data.loc[i]\n",
        "    scores, indices = model.test(row, 3)\n",
        "    # string = f\"{row['PRODUCT_ID']},{indices[0]}\"\n",
        "    # out.append({\"PRODUCT_ID,BROWSE_NODE_ID\": string})\n",
        "    out.append({\"PRODUCT_ID\" : row['PRODUCT_ID'], \"BROWSE_NODE_ID\" : indices[0]})\n",
        "    \n",
        "print()\n",
        "\n",
        "prediction = pd.DataFrame.from_dict(out)\n",
        "# prediction.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....................................................................................................\n",
            "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtwYf0Ghae8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2ad3c760-9769-4878-d32f-3b930924e95c"
      },
      "source": [
        "prediction.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>BROWSE_NODE_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>852</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PRODUCT_ID  BROWSE_NODE_ID\n",
              "0           1             435\n",
              "1           2             474\n",
              "2           3              18\n",
              "3           4             125\n",
              "4           5             852"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}